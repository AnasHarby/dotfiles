## Image Classification

- **L1 norm Distance: ** $d_1(I_1, I_2) = \sum |I_1^p - I_2^p|$ 

- **L2 norm Distance**: $d_2(I_1, I_2) = \sum \sqrt{\sum (I_1^p - I_2^p)^2}$

- **Nearest Neighbor Classifier: **

  ```python
  import numpy as np

  class NearestNeighbor(object):
    def __init__(self):
      pass

    def train(self, X, y):
      """ X is N x D where each row is an example. Y is 1-dimension of size N """
      # the nearest neighbor classifier simply remembers all the training data
      self.Xtr = X
      self.ytr = y

    def predict(self, X):
      """ X is N x D where each row is an example we wish to predict label for """
      num_test = X.shape[0]
      # lets make sure that the output type matches the input type
      Ypred = np.zeros(num_test, dtype = self.ytr.dtype)

      # loop over all test rows
      for i in xrange(num_test):
        # find the nearest training image to the i'th test image
        # using the L1 distance (sum of absolute value differences)
        distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)
        min_index = np.argmin(distances) # get the index with smallest distance
        Ypred[i] = self.ytr[min_index] # predict the label of the nearest example

      return Ypred

  ```

## Linear Classification

**Overview**. We are now going to develop a more powerful approach to image classification that we will eventually 	naturally extend to entire Neural Networks and Convolutional Neural Networks. The approach will have two major components: a **score function** that maps the raw data to class scores, and a **loss function** that quantifies the agreement between the predicted scores and the ground truth labels. We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function.
$$
f(x_i,W,b)=W x_i + b
$$
In the above equation, we are assuming that the image $x_i$ has all of its pixels flattened out to a single column vector of shape [D x 1]. The matrix **W** (of size [K x D]), and the vector **b** (of size [K x 1]) are the **parameters** of the function. In CIFAR-10, $x_i$ contains all pixels in the i-th image flattened into a single [3072 x 1] column, **W** is [10 x 3072] and **b** is [10 x 1], so 3072 numbers come into the function (the raw pixel values) and 10 numbers come out (the class scores). The parameters in **W** are often called the **weights**, and **b** is called the **bias vector** because it influences the output scores, but without interacting with the actual data $x_i$. However, you will often hear people use the terms *weights* and *parameters* interchangeably.

#### Bias Trick

$$
f(x_i,W)=Wx_i
$$

With our CIFAR-10 example, $xi$ is now [3073 x 1] instead of [3072 x 1] - (with the extra dimension holding the constant 1), and **W** is now [10 x 3073] instead of [10 x 3072]. The extra column that **W** now corresponds to the bias **b**.

#### Multiclass SVM 

There are several ways to define the details of the loss function. As a first example we will first develop a commonly used loss called the **Multiclass Support Vector Machine** (SVM) loss. The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin **Δ**. Notice that it’s sometimes helpful to anthropomorphise the loss functions as we did above: The SVM “wants” a certain outcome in the sense that the outcome would yield a lower loss (which is good).

Let’s now get more precise. Recall that for the i-th example we are given the pixels of image $x_i$ and the label $y_i$ that specifies the index of the correct class. The score function takes the pixels and computes the vector f(xi,W)f(xi,W)of class scores, which we will abbreviate to ss (short for scores). For example, the score for the j-th class is the j-th element: sj=f(xi,W)jsj=f(xi,W)j. The Multiclass SVM loss for the i-th example is then formalized as follows:

Li=∑j≠yimax(0,sj−syi+Δ)

